{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Aninda\\miniconda3\\envs\\geo-clip\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from geopy.distance import geodesic as GD\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "from config import cfg\n",
    "\n",
    "from geo_clip import GeoCLIP, img_val_transform\n",
    "from dataset.dataset import GeoCLIPDataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance_accuracy(targets, preds, dis=2500, gps_gallery=None):\n",
    "    total = len(targets)\n",
    "    correct = 0\n",
    "    gd_avg = 0\n",
    "\n",
    "    for i in range(total):\n",
    "        gd = GD(gps_gallery[preds[i]], targets[i]).km\n",
    "        gd_avg += gd\n",
    "        if gd <= dis:\n",
    "            correct += 1\n",
    "\n",
    "    gd_avg /= total\n",
    "    return correct / total, gd_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GeoCLIP(\n",
       "  (image_encoder): ImageEncoder(\n",
       "    (CLIP): CLIPModel(\n",
       "      (text_model): CLIPTextTransformer(\n",
       "        (embeddings): CLIPTextEmbeddings(\n",
       "          (token_embedding): Embedding(49408, 768)\n",
       "          (position_embedding): Embedding(77, 768)\n",
       "        )\n",
       "        (encoder): CLIPEncoder(\n",
       "          (layers): ModuleList(\n",
       "            (0-11): 12 x CLIPEncoderLayer(\n",
       "              (self_attn): CLIPAttention(\n",
       "                (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              )\n",
       "              (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): CLIPMLP(\n",
       "                (activation_fn): QuickGELUActivation()\n",
       "                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              )\n",
       "              (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (vision_model): CLIPVisionTransformer(\n",
       "        (embeddings): CLIPVisionEmbeddings(\n",
       "          (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
       "          (position_embedding): Embedding(257, 1024)\n",
       "        )\n",
       "        (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder): CLIPEncoder(\n",
       "          (layers): ModuleList(\n",
       "            (0-23): 24 x CLIPEncoderLayer(\n",
       "              (self_attn): CLIPAttention(\n",
       "                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): CLIPMLP(\n",
       "                (activation_fn): QuickGELUActivation()\n",
       "                (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              )\n",
       "              (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (visual_projection): Linear(in_features=1024, out_features=768, bias=False)\n",
       "      (text_projection): Linear(in_features=768, out_features=768, bias=False)\n",
       "    )\n",
       "    (mlp): Sequential(\n",
       "      (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=768, out_features=512, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (location_encoder): LocationEncoder(\n",
       "    (LocEnc0): LocationEncoderCapsule(\n",
       "      (capsule): Sequential(\n",
       "        (0): GaussianEncoding()\n",
       "        (1): Linear(in_features=512, out_features=1024, bias=True)\n",
       "        (2): ReLU()\n",
       "        (3): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (4): ReLU()\n",
       "        (5): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (6): ReLU()\n",
       "      )\n",
       "      (head): Sequential(\n",
       "        (0): Linear(in_features=1024, out_features=512, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (LocEnc1): LocationEncoderCapsule(\n",
       "      (capsule): Sequential(\n",
       "        (0): GaussianEncoding()\n",
       "        (1): Linear(in_features=512, out_features=1024, bias=True)\n",
       "        (2): ReLU()\n",
       "        (3): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (4): ReLU()\n",
       "        (5): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (6): ReLU()\n",
       "      )\n",
       "      (head): Sequential(\n",
       "        (0): Linear(in_features=1024, out_features=512, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (LocEnc2): LocationEncoderCapsule(\n",
       "      (capsule): Sequential(\n",
       "        (0): GaussianEncoding()\n",
       "        (1): Linear(in_features=512, out_features=1024, bias=True)\n",
       "        (2): ReLU()\n",
       "        (3): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (4): ReLU()\n",
       "        (5): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (6): ReLU()\n",
       "      )\n",
       "      (head): Sequential(\n",
       "        (0): Linear(in_features=1024, out_features=512, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (criterion): Contrastive_Loss()\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GeoCLIP(cfg)\n",
    "state_dict = torch.load(cfg.MODEL.CHECKPOINT_PATH, map_location='cpu')\n",
    "model.load_state_dict(state_dict['state_dict'])\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading image paths and coordinates: 4536it [00:00, 26360.81it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = GeoCLIPDataModule(dataset_file=cfg.DATA.EVAL_DATASET_FILE, transform=img_val_transform())\n",
    "\n",
    "val_dataloader = DataLoader(dataset, pin_memory=True, batch_size=cfg.VALIDATION.BATCH_SIZE, shuffle=False, num_workers=cfg.VALIDATION.NUM_WORKERS, persistent_workers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 284/284 [35:35<00:00,  7.52s/it]\n"
     ]
    }
   ],
   "source": [
    "gps_gallery = model.gps_gallery.to(device)\n",
    "\n",
    "preds = []\n",
    "targets = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in tqdm(val_dataloader, desc=\"Evaluating\"):\n",
    "        labels = labels.numpy()\n",
    "        imgs = imgs.numpy()\n",
    "\n",
    "        imgs = torch.tensor(imgs, dtype=torch.float32, device=device)\n",
    "        labels = torch.tensor(labels, dtype=torch.float32, device=device)\n",
    "\n",
    "        logits_per_image = model(imgs, gps_gallery)\n",
    "        probs = logits_per_image.softmax(dim=-1)\n",
    "\n",
    "        output = torch.argmax(probs, dim=-1)\n",
    "\n",
    "        preds.append(output.cpu().numpy())\n",
    "        targets.append(labels.cpu().numpy())\n",
    "\n",
    "preds = np.concatenate(preds, axis=0)\n",
    "targets = np.concatenate(targets, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy at 2500 km: 67.77\n",
      "Accuracy at 750 km: 44.44\n",
      "Accuracy at 200 km: 21.94\n",
      "Accuracy at 25 km: 9.08\n",
      "Accuracy at 1 km: 1.21\n"
     ]
    }
   ],
   "source": [
    "distance_thresholds = [2500, 750, 200, 25, 1] # km\n",
    "accuracy_results = {}\n",
    "for dis in distance_thresholds:\n",
    "    acc, avg_distance_error = distance_accuracy(targets, preds, dis, gps_gallery)\n",
    "    print(f\"Accuracy at {dis} km: {round(acc*100, 2)}\")\n",
    "    accuracy_results[f'acc_{dis}_km'] = acc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geo-clip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
